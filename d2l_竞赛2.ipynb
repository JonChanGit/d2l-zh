{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "mount_file_id": "1-W_nXTmlkcPrRDasVTU3_AQTgzq_NJGa",
      "authorship_tag": "ABX9TyMSfgO0zQ4XffmVIfBdDHxO",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JonChanGit/d2l-zh/blob/master/d2l_%E7%AB%9E%E8%B5%9B2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SBqPOIBUIYXS"
      },
      "outputs": [],
      "source": [
        "! pip install d2l mxnet torch torchvision torchaudio\n",
        "! nvidia-smi\n",
        "! unzip /content/drive/MyDrive/sample/classify-leaves.zip  -d /content/classify-leaves/"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader, Dataset\n",
        "from torchvision import transforms\n",
        "import os\n",
        "from PIL import Image\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "from d2l import torch as d2l\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.nn import functional as F\n",
        "\n",
        "from sklearn.preprocessing import LabelEncoder"
      ],
      "metadata": {
        "id": "HqE_uE5Yw5Um"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_label_map(csv_file):\n",
        "  \"\"\"\n",
        "  映射标签和分类序号关系\n",
        "  \"\"\"\n",
        "  labels = pd.read_csv(csv_file).iloc[:, 1].values\n",
        "  unique_labels, counts = np.unique(labels, return_counts=True)\n",
        "  class2num_map: dict[str, int] = dict(zip(unique_labels, range(len(unique_labels))))\n",
        "  num2class_map: dict[int, str] = dict(zip(range(len(unique_labels)), unique_labels))\n",
        "  return class2num_map ,num2class_map\n",
        "\n",
        "class2num_map ,num2class_map = load_label_map('/content/classify-leaves/train.csv')\n",
        "\n",
        "lr, num_epochs, batch_size = 0.05, 10, 1"
      ],
      "metadata": {
        "id": "aslnlnabJ_DO"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 残差块\n",
        "class Residual(nn.Module):\n",
        "    def __init__(self, input_channels, num_channels,\n",
        "                 use_1x1conv=False, strides=1):\n",
        "        # num_channel 输出通道数\n",
        "        # use_1x1conv 是否使用 1*1卷积层\n",
        "        # stride。步长\n",
        "        super().__init__()\n",
        "        self.conv1 = nn.Conv2d(input_channels, num_channels,\n",
        "                               kernel_size=3, padding=1, stride=strides)\n",
        "        self.conv2 = nn.Conv2d(num_channels, num_channels,\n",
        "                               kernel_size=3, padding=1)\n",
        "        if use_1x1conv:\n",
        "            self.conv3 = nn.Conv2d(input_channels, num_channels,\n",
        "                                   kernel_size=1, stride=strides)\n",
        "        else:\n",
        "            self.conv3 = None\n",
        "        # 批量归一化\n",
        "        self.bn1 = nn.BatchNorm2d(num_channels)\n",
        "        self.bn2 = nn.BatchNorm2d(num_channels)\n",
        "\n",
        "    def forward(self, X):\n",
        "        Y = F.relu(self.bn1(self.conv1(X)))\n",
        "        Y = self.bn2(self.conv2(Y))\n",
        "        if self.conv3:\n",
        "            # 对输入直接使用 1*1 卷积改写输入形状，以便更改残差块输出形状\n",
        "            X = self.conv3(X)\n",
        "        # 残差链接： 输出 + 输入\n",
        "        Y += X\n",
        "        return F.relu(Y)\n",
        "\n",
        "def getNet():\n",
        "  # 定义模型\n",
        "  # b1 卷积（输出通道提升到64）+归一化+激活+最大池化\n",
        "  b1 = nn.Sequential(nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3),\n",
        "                    nn.BatchNorm2d(64), nn.ReLU(),\n",
        "                    nn.MaxPool2d(kernel_size=3, stride=2, padding=1))\n",
        "\n",
        "\n",
        "  def resnet_block(input_channels, num_channels, num_residuals,\n",
        "                  first_block=False):\n",
        "      # num_residual 残差块数量\n",
        "      # first_block b1中已经高宽减半了，所以此标识表示不减半，通道数不变\n",
        "      blk = []\n",
        "      for i in range(num_residuals):\n",
        "          if i == 0 and not first_block:\n",
        "              blk.append(Residual(input_channels, num_channels,\n",
        "                                  use_1x1conv=True, strides=2))\n",
        "          else:\n",
        "              blk.append(Residual(num_channels, num_channels))\n",
        "      return blk\n",
        "\n",
        "  # b2 残差stage\n",
        "  b2 = nn.Sequential(*resnet_block(64, 64, 2, first_block=True))\n",
        "  b3 = nn.Sequential(*resnet_block(64, 128, 2))\n",
        "  b4 = nn.Sequential(*resnet_block(128, 256, 2))\n",
        "  b5 = nn.Sequential(*resnet_block(256, 512, 2))\n",
        "\n",
        "  #\n",
        "  net = nn.Sequential(b1, b2, b3, b4, b5,\n",
        "                      nn.AdaptiveAvgPool2d((1,1)),\n",
        "                      nn.Flatten(), nn.Linear(512, len(class2num_map)))\n",
        "  return net"
      ],
      "metadata": {
        "id": "yduMLf17JnKt"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class LeavesDataset(Dataset):\n",
        "  def __init__(self, root_dir, csv_file, train=True):\n",
        "      \"\"\"\n",
        "      csv_file (string): csv 文件路径\n",
        "      root_dir (string): 图像文件的目录路径\n",
        "      transform (callable, optional): 一个可选的转换函数，用于对样本进行处理\n",
        "      train (bool, optional): 是训练集还是测试集的标志\n",
        "      \"\"\"\n",
        "      self.data_frame = pd.read_csv(csv_file)\n",
        "      self.root_dir = root_dir\n",
        "      trans = [transforms.ToTensor()]\n",
        "      trans.insert(0, transforms.Resize(96))\n",
        "      self.train = train\n",
        "      self.transform = transforms.Compose(trans)\n",
        "      self.label_transform = class2num_map\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    if self.train:\n",
        "        img_name = os.path.join(self.root_dir, self.data_frame.iloc[idx, 0])\n",
        "        image = Image.open(img_name)\n",
        "        label = self.data_frame.iloc[idx, 1]\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "        #print('???')\n",
        "        return image, self.label_transform[label]\n",
        "    else:\n",
        "        img_name = os.path.join(self.root_dir, self.data_frame.iloc[idx, 0])\n",
        "        image = Image.open(img_name)\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "        return image\n",
        "\n",
        "  def __len__(self):\n",
        "    # 返回数据集中图像的数量\n",
        "    return len(self.data_frame)\n",
        "\n",
        ""
      ],
      "metadata": {
        "id": "-VURXVYcPb-u"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_accuracy_gpu(net, data_iter, device=None):\n",
        "    \"\"\"使用GPU计算模型在数据集上的精度\n",
        "\n",
        "    Defined in :numref:`sec_lenet`\"\"\"\n",
        "    if isinstance(net, nn.Module):\n",
        "        net.eval()  # 设置为评估模式\n",
        "        if not device:\n",
        "            device = next(iter(net.parameters())).device\n",
        "    # 正确预测的数量，总预测的数量\n",
        "    metric = d2l.Accumulator(2)\n",
        "    with torch.no_grad():\n",
        "        for X, y in data_iter:\n",
        "            if isinstance(X, list):\n",
        "                # BERT微调所需的（之后将介绍）\n",
        "                X = [x.to(device) for x in X]\n",
        "            else:\n",
        "                X = X.to(device)\n",
        "            y = y.to(device)\n",
        "            metric.add(d2l.accuracy(net(X), y), d2l.size(y))\n",
        "    return metric[0] / metric[1]\n",
        "\n",
        "def train(net, train_iter, test_iter, num_epochs, lr, device):\n",
        "  def init_weights(m):\n",
        "        if type(m) == nn.Linear or type(m) == nn.Conv2d:\n",
        "            nn.init.xavier_uniform_(m.weight)\n",
        "  net.apply(init_weights)\n",
        "  # print('training on', device)\n",
        "  net.to(device)\n",
        "  # 优化器\n",
        "  optimizer = torch.optim.SGD(net.parameters(), lr=lr)\n",
        "  # 损失函数\n",
        "  loss = nn.CrossEntropyLoss()\n",
        "  # 画图\n",
        "  animator = d2l.Animator(xlabel='epoch', xlim=[1, num_epochs],\n",
        "                          legend=['train loss', 'train acc', 'test acc'])\n",
        "  timer, num_batches = d2l.Timer(), len(train_iter)\n",
        "  for epoch in range(num_epochs):\n",
        "    # 训练损失之和，训练准确率之和，样本数\n",
        "    metric = d2l.Accumulator(3)\n",
        "    net.train()\n",
        "    for i, (X, y) in enumerate(train_iter):\n",
        "      timer.start()\n",
        "      optimizer.zero_grad()\n",
        "      # 此处的X，y受batch_size影响，一次获取一批数据\n",
        "      X, y = X.to(device), y.to(device)\n",
        "      # 本次预测值\n",
        "      y_hat = net(X)\n",
        "      #print(y_hat, y)\n",
        "      # 计算损失\n",
        "      l = loss(y_hat, y)\n",
        "      # 计算梯度并优化\n",
        "      l.backward()\n",
        "      optimizer.step()\n",
        "      with torch.no_grad():\n",
        "        # 临时禁用梯度\n",
        "        metric.add(l * X.shape[0], d2l.accuracy(y_hat, y), X.shape[0])\n",
        "      timer.stop()\n",
        "      train_l = metric[0] / metric[2]\n",
        "      train_acc = metric[1] / metric[2]\n",
        "      if (i + 1) % (num_batches // 5) == 0 or i == num_batches - 1:\n",
        "          animator.add(epoch + (i + 1) / num_batches,\n",
        "                        (train_l, train_acc, None))\n",
        "  test_acc = evaluate_accuracy_gpu(net, test_iter)\n",
        "  animator.add(epoch + 1, (None, None, test_acc))\n",
        "  print(f'loss {train_l:.3f}, train acc {train_acc:.3f}, '\n",
        "        f'test acc {test_acc:.3f}')\n",
        "  print(f'{metric[2] * num_epochs / timer.sum():.1f} examples/sec '\n",
        "        f'on {str(device)}')"
      ],
      "metadata": {
        "id": "-gOLE-FmTVIZ"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "net = getNet()\n",
        "train_set = DataLoader(LeavesDataset('/content/classify-leaves/', '/content/classify-leaves/train.csv'), batch_size, shuffle=True, num_workers=4)\n",
        "test_set = DataLoader(LeavesDataset('/content/classify-leaves/', '/content/classify-leaves/test.csv'), batch_size, shuffle=True, num_workers=4)\n",
        "\n",
        "train(net, train_set, test_set, num_epochs, lr, d2l.try_gpu())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t3Ni4N4vU_Kz",
        "outputId": "d8f9dae9-4800-4840-d29c-8e05ad578fb0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:558: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:558: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_iter, test_iter = d2l.load_data_fashion_mnist(1, resize=96)\n",
        "for i, (X, y) in enumerate(train_iter):\n",
        "  break\n",
        "X.shape, y"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jT9fAJyvfcny",
        "outputId": "1488a26a-61bf-4991-8707-3fc996f389c6"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:558: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([1, 1, 96, 96]), tensor([8]))"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "net"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FYMeOp7xp2iF",
        "outputId": "a70243e0-2125-49fd-877c-50da47f3bd7f"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Sequential(\n",
              "  (0): Sequential(\n",
              "    (0): Conv2d(1, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3))\n",
              "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (2): ReLU()\n",
              "    (3): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
              "  )\n",
              "  (1): Sequential(\n",
              "    (0): Residual(\n",
              "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (1): Residual(\n",
              "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "  )\n",
              "  (2): Sequential(\n",
              "    (0): Residual(\n",
              "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
              "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (conv3): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2))\n",
              "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (1): Residual(\n",
              "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "  )\n",
              "  (3): Sequential(\n",
              "    (0): Residual(\n",
              "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (conv3): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2))\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (1): Residual(\n",
              "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "  )\n",
              "  (4): Sequential(\n",
              "    (0): Residual(\n",
              "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
              "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (conv3): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2))\n",
              "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (1): Residual(\n",
              "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "  )\n",
              "  (5): AdaptiveAvgPool2d(output_size=(1, 1))\n",
              "  (6): Flatten(start_dim=1, end_dim=-1)\n",
              "  (7): Linear(in_features=512, out_features=10, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "d2l.try_gpu()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hAMntQqPuOVf",
        "outputId": "6232cedf-eda9-4355-c146-a53c27ce0093"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda', index=0)"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    }
  ]
}